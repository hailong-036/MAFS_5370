{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "# Define device for PyTorch (use GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Board size (12x12 grid)\n",
    "BOARD_SIZE = 12\n",
    "\n",
    "# Size of each cell in pixels for GUI\n",
    "CELL_SIZE = 40\n",
    "\n",
    "# Color definitions for GUI elements\n",
    "COLORS = {\n",
    "    \"bg\": \"#F0D9B5\",      # Background color\n",
    "    \"grid\": \"#000000\",    # Grid line color\n",
    "    \"valid\": \"#FFFFFF\",   # Color for valid/empty cells\n",
    "    \"p1\": \"#2C5F2D\",     # Player 1 piece color (green)\n",
    "    \"p2\": \"#B1624E\"      # Player 2 piece color (red)\n",
    "}\n",
    "\n",
    "# Define valid move zones (cross-shaped regions, excluding corners)\n",
    "CROSS_ZONES = [\n",
    "    (4, 8, 4, 8),   # Central zone\n",
    "    (0, 4, 4, 8),   # Left zone\n",
    "    (8, 12, 4, 8),  # Right zone\n",
    "    (4, 8, 0, 4),   # Top zone\n",
    "    (4, 8, 8, 12)   # Bottom zone\n",
    "]\n",
    "\n",
    "class State:\n",
    "    \"\"\"Class to manage the game state, board, and logic.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the game state.\"\"\"\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the board to empty, clear winner and game end status.\"\"\"\n",
    "        self.data = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)  # 12x12 board (0: empty, 1: P1, -1: P2)\n",
    "        self.winner = None  # Winner (1, -1, or 0 for draw)\n",
    "        self.end = False    # Game end flag\n",
    "        \n",
    "    def is_valid_position(self, row, col):\n",
    "        \"\"\"Check if a position is within valid cross zones and empty.\"\"\"\n",
    "        for x1, x2, y1, y2 in CROSS_ZONES:\n",
    "            if x1 <= row < x2 and y1 <= col < y2:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    # def check_consecutive(self, player, count=3):\n",
    "    #     directions = [(0, 1), (1, 0), (1, 1), (1, -1)]\n",
    "    #     for i in range(BOARD_SIZE):\n",
    "    #         for j in range(BOARD_SIZE):\n",
    "    #             for dx, dy in directions:\n",
    "    #                 if 0 <= i + dx*(count-1) < BOARD_SIZE and 0 <= j + dy*(count-1) < BOARD_SIZE:\n",
    "    #                     seq = [self.data[i + dx*k][j + dy*k] for k in range(count)]\n",
    "    #                     if all(cell == player for cell in seq):\n",
    "    #                         return True\n",
    "    #     return False\n",
    "    \n",
    "    # def check_consecutive(self, player, count, directions=None):\n",
    "    #     if torch.sum(self.data == player).item() < count:  # Early exit if not enough pieces\n",
    "    #         return False\n",
    "        \n",
    "    #     # Default to all directions if not specified\n",
    "    #     if directions is None:\n",
    "    #         directions = [(0, 1), (1, 0), (1, 1), (1, -1)]\n",
    "        \n",
    "    #     for i in range(BOARD_SIZE):\n",
    "    #         for j in range(BOARD_SIZE):\n",
    "    #             for dx, dy in directions:\n",
    "    #                 if 0 <= i + dx*(count-1) < BOARD_SIZE and 0 <= j + dy*(count-1) < BOARD_SIZE:\n",
    "    #                     seq = [self.data[i + dx*k, j + dy*k].item() for k in range(count)]\n",
    "    #                     if all(cell == player for cell in seq):\n",
    "    #                         return True\n",
    "    #     return False\n",
    "    def check_consecutive(self, player, count, directions=None, row=None, col=None):\n",
    "        \"\"\"\n",
    "        Check for 'count' consecutive pieces for a player in specified directions.\n",
    "        Returns (has_consecutive, can_extend) to indicate sequence existence and extendability.\n",
    "        \"\"\"\n",
    "        # Set default directions if none provided (horizontal, vertical, diagonals)\n",
    "        if directions is None:\n",
    "            directions = [(0, 1), (1, 0), (1, 1), (1, -1)]\n",
    "        \n",
    "        # Restrict check to a local region around (row, col) if provided\n",
    "        if row is not None and col is not None:\n",
    "            i_range = range(max(0, row-count+1), min(BOARD_SIZE, row+count))\n",
    "            j_range = range(max(0, col-count+1), min(BOARD_SIZE, col+count))\n",
    "        else:\n",
    "            i_range = range(BOARD_SIZE)\n",
    "            j_range = range(BOARD_SIZE)\n",
    "        \n",
    "        has_consecutive = False  # Flag for found sequence\n",
    "        can_extend = False       # Flag for extendable sequence\n",
    "        \n",
    "        # Iterate over board positions and directions\n",
    "        for i in i_range:\n",
    "            for j in j_range:\n",
    "                for dx, dy in directions:\n",
    "                    # Check if sequence fits within board bounds\n",
    "                    if 0 <= i + dx*(count-1) < BOARD_SIZE and 0 <= j + dy*(count-1) < BOARD_SIZE:\n",
    "                        # Extract sequence of 'count' cells\n",
    "                        seq = [self.data[i + dx*k, j + dy*k].item() for k in range(count)]\n",
    "                        if all(cell == player for cell in seq):\n",
    "                            has_consecutive = True\n",
    "                            # Check extendability for 3-consecutive or diagonal 4-consecutive\n",
    "                            if count == 3 or (count == 4 and (dx, dy) in [(1, 1), (1, -1)]):\n",
    "                                extendable = False\n",
    "                                # Check left/up end\n",
    "                                left_i, left_j = i - dx, j - dy\n",
    "                                # Check right/down end\n",
    "                                right_i, right_j = i + dx*count, j + dy*count\n",
    "                                # Sequence is extendable if at least one end is empty\n",
    "                                if (0 <= left_i < BOARD_SIZE and 0 <= left_j < BOARD_SIZE and\n",
    "                                    self.data[left_i, left_j] == 0):\n",
    "                                    extendable = True\n",
    "                                elif (0 <= right_i < BOARD_SIZE and 0 <= right_j < BOARD_SIZE and\n",
    "                                      self.data[right_i, right_j] == 0):\n",
    "                                    extendable = True\n",
    "                                if extendable:\n",
    "                                    can_extend = True\n",
    "                            else:\n",
    "                                can_extend = True  # 2-consecutive always extendable\n",
    "                            if has_consecutive and can_extend:\n",
    "                                return True, True\n",
    "        return has_consecutive, can_extend\n",
    "    \n",
    "    # def update_state(self, row, col, player):\n",
    "    #     if self.is_valid_position(row, col) and self.data[row][col] == 0:\n",
    "    #         self.data[row][col] = player\n",
    "    #         reward = 0\n",
    "    #         self.check_winner()\n",
    "            \n",
    "    #         if self.end:\n",
    "    #             reward = 1.0 if self.winner == player else -1.0\n",
    "    #         elif self.check_consecutive(-player, 3):\n",
    "    #             reward += -0.2\n",
    "    #         elif self.check_consecutive(-player, 2):\n",
    "    #             reward += -0.1\n",
    "            \n",
    "    #         return True, reward\n",
    "    #     return False, 0\n",
    "\n",
    "    # def update_state(self, row, col, player):\n",
    "    #     if self.is_valid_position(row, col) and self.data[row][col] == 0:\n",
    "    #         self.data[row][col] = player\n",
    "    #         reward = 0.0\n",
    "    #         reward_scale = 0.5  # Scale factor to balance rewards\n",
    "            \n",
    "    #         # Check game end and winner\n",
    "    #         self.check_winner()\n",
    "    #         if self.end:\n",
    "    #             reward = 1.0 if self.winner == player else -1.0 if self.winner == -player else 0.0\n",
    "    #             return True, reward\n",
    "            \n",
    "    #         # Positive rewards for player's consecutive pieces\n",
    "    #         if self.check_consecutive(player, 3):\n",
    "    #             reward += 0.15 * reward_scale  # Reward for 3 consecutive pieces\n",
    "    #         elif self.check_consecutive(player, 2):\n",
    "    #             reward += 0.05 * reward_scale  # Reward for 2 consecutive pieces\n",
    "                \n",
    "    #         # Negative rewards for opponent's consecutive pieces\n",
    "    #         if self.check_consecutive(-player, 3):\n",
    "    #             reward += -0.2 * reward_scale  # Penalty for opponent's 3 consecutive pieces\n",
    "    #         elif self.check_consecutive(-player, 2):\n",
    "    #             reward += -0.1 * reward_scale  # Penalty for opponent's 2 consecutive pieces\n",
    "            \n",
    "    #         return True, reward\n",
    "    #     return False, 0.0\n",
    "    # def update_state(self, row, col, player):\n",
    "    #     if self.is_valid_position(row, col) and self.data[row, col] == 0:\n",
    "    #         # Check consecutive pieces before move\n",
    "    #         hv_directions = [(0, 1), (1, 0)]  # Horizontal/Vertical\n",
    "    #         prev_player_3 = self.check_consecutive(player, 3, hv_directions)\n",
    "    #         prev_player_2 = self.check_consecutive(player, 2, hv_directions) and not prev_player_3\n",
    "    #         prev_opponent_3 = self.check_consecutive(-player, 3, hv_directions)\n",
    "    #         prev_opponent_2 = self.check_consecutive(-player, 2, hv_directions) and not prev_opponent_3\n",
    "\n",
    "    #         # Update board\n",
    "    #         self.data[row, col] = player\n",
    "    #         reward = 0.0\n",
    "    #         reward_scale = 0.5\n",
    "            \n",
    "    #         # Check game end and winner\n",
    "    #         self.check_winner()\n",
    "    #         if self.end:\n",
    "    #             reward = 1.0 if self.winner == player else -1.0 if self.winner == -player else 0.0\n",
    "    #             return True, reward\n",
    "            \n",
    "    #         # Check consecutive pieces after move\n",
    "    #         curr_player_3 = self.check_consecutive(player, 3, hv_directions)\n",
    "    #         curr_player_2 = self.check_consecutive(player, 2, hv_directions) and not curr_player_3\n",
    "    #         curr_opponent_3 = self.check_consecutive(-player, 3, hv_directions)\n",
    "    #         curr_opponent_2 = self.check_consecutive(-player, 2, hv_directions) and not curr_opponent_3\n",
    "            \n",
    "    #         # Positive rewards for newly formed consecutive pieces\n",
    "    #         if curr_player_3 and not prev_player_3:\n",
    "    #             reward += 0.10 * reward_scale  # Reward for new 3 consecutive pieces\n",
    "    #         elif curr_player_2 and not prev_player_2:\n",
    "    #             reward += 0.05 * reward_scale  # Reward for new 2 consecutive pieces\n",
    "                \n",
    "    #         # Negative rewards for opponent's newly formed consecutive pieces\n",
    "    #         if curr_opponent_3 and not prev_opponent_3:\n",
    "    #             reward += -0.15 * reward_scale  # Penalty for opponent's new 3 consecutive pieces\n",
    "    #         elif curr_opponent_2 and not prev_opponent_2:\n",
    "    #             reward += -0.10 * reward_scale  # Penalty for opponent's new 2 consecutive pieces\n",
    "            \n",
    "    #         return True, reward\n",
    "    #     return False, 0.0\n",
    "    \n",
    "    def update_state(self, row, col, player):\n",
    "        \"\"\"Update board with player's move, calculate rewards, and check for winner.\"\"\"\n",
    "        if self.is_valid_position(row, col) and self.data[row, col] == 0:\n",
    "            # Check consecutive pieces before the move\n",
    "            hv_directions = [(0, 1), (1, 0)]  # Horizontal/Vertical directions\n",
    "            diag_directions = [(1, 1), (1, -1)]  # Diagonal directions\n",
    "            # Player's pre-move consecutive checks\n",
    "            prev_player_3_hv, prev_player_3_hv_ext = self.check_consecutive(player, 3, hv_directions, row, col)\n",
    "            prev_player_2_hv, prev_player_2_hv_ext = self.check_consecutive(player, 2, hv_directions, row, col)\n",
    "            prev_player_3_diag, prev_player_3_diag_ext = self.check_consecutive(player, 3, diag_directions, row, col)\n",
    "            prev_player_2_diag, prev_player_2_diag_ext = self.check_consecutive(player, 2, diag_directions, row, col)\n",
    "            prev_player_4_diag, prev_player_4_diag_ext = self.check_consecutive(player, 4, diag_directions, row, col)\n",
    "            # Opponent's pre-move consecutive checks\n",
    "            prev_opponent_3_hv, prev_opponent_3_hv_ext = self.check_consecutive(-player, 3, hv_directions, row, col)\n",
    "            prev_opponent_2_hv, prev_opponent_2_hv_ext = self.check_consecutive(-player, 2, hv_directions, row, col)\n",
    "            prev_opponent_3_diag, prev_opponent_3_diag_ext = self.check_consecutive(-player, 3, diag_directions, row, col)\n",
    "            prev_opponent_2_diag, prev_opponent_2_diag_ext = self.check_consecutive(-player, 2, diag_directions, row, col)\n",
    "            prev_opponent_4_diag, prev_opponent_4_diag_ext = self.check_consecutive(-player, 4, diag_directions, row, col)\n",
    "\n",
    "            # Place player's piece on the board\n",
    "            self.data[row, col] = player\n",
    "            reward = 0.0\n",
    "            reward_scale = 0.5  # Scale factor for balancing rewards\n",
    "            \n",
    "            # Check for game end and winner\n",
    "            self.check_winner()\n",
    "            if self.end:\n",
    "                reward = 1.0 if self.winner == player else -1.0 if self.winner == -player else 0.0\n",
    "                return True, reward\n",
    "            \n",
    "            # Check consecutive pieces after the move\n",
    "            curr_player_3_hv, curr_player_3_hv_ext = self.check_consecutive(player, 3, hv_directions, row, col)\n",
    "            curr_player_2_hv, curr_player_2_hv_ext = self.check_consecutive(player, 2, hv_directions, row, col)\n",
    "            curr_player_3_diag, curr_player_3_diag_ext = self.check_consecutive(player, 3, diag_directions, row, col)\n",
    "            curr_player_2_diag, curr_player_2_diag_ext = self.check_consecutive(player, 2, diag_directions, row, col)\n",
    "            curr_player_4_diag, curr_player_4_diag_ext = self.check_consecutive(player, 4, diag_directions, row, col)\n",
    "            curr_opponent_3_hv, curr_opponent_3_hv_ext = self.check_consecutive(-player, 3, hv_directions, row, col)\n",
    "            curr_opponent_2_hv, curr_opponent_2_hv_ext = self.check_consecutive(-player, 2, hv_directions, row, col)\n",
    "            curr_opponent_3_diag, curr_opponent_3_diag_ext = self.check_consecutive(-player, 3, diag_directions, row, col)\n",
    "            curr_opponent_2_diag, curr_opponent_2_diag_ext = self.check_consecutive(-player, 2, diag_directions, row, col)\n",
    "            curr_opponent_4_diag, curr_opponent_4_diag_ext = self.check_consecutive(-player, 4, diag_directions, row, col)\n",
    "            \n",
    "            # Assign positive rewards for player's new extendable consecutive pieces\n",
    "            if curr_player_3_hv and not prev_player_3_hv and curr_player_3_hv_ext:\n",
    "                reward += 0.15 * reward_scale  # Reward for new extendable 3-consecutive (horizontal/vertical)\n",
    "            elif curr_player_2_hv and not prev_player_2_hv and curr_player_2_hv_ext:\n",
    "                reward += 0.05 * reward_scale  # Reward for new 2-consecutive (horizontal/vertical)\n",
    "            if curr_player_4_diag and not prev_player_4_diag and curr_player_4_diag_ext:\n",
    "                reward += 0.12 * reward_scale  # Reward for new extendable 4-consecutive (diagonal)\n",
    "            elif curr_player_3_diag and not prev_player_3_diag and curr_player_3_diag_ext:\n",
    "                reward += 0.08 * reward_scale  # Reward for new extendable 3-consecutive (diagonal)\n",
    "            elif curr_player_2_diag and not prev_player_2_diag and curr_player_2_diag_ext:\n",
    "                reward += 0.03 * reward_scale  # Reward for new 2-consecutive (diagonal)\n",
    "                \n",
    "            # Assign negative rewards for opponent's new extendable consecutive pieces\n",
    "            if curr_opponent_3_hv and not prev_opponent_3_hv and curr_opponent_3_hv_ext:\n",
    "                reward += -0.18 * reward_scale  # Penalty for opponent's new extendable 3-consecutive (horizontal/vertical)\n",
    "            elif curr_opponent_2_hv and not prev_opponent_2_hv and curr_opponent_2_hv_ext:\n",
    "                reward += -0.05 * reward_scale  # Penalty for opponent's new 2-consecutive (horizontal/vertical)\n",
    "            elif curr_opponent_4_diag and not prev_opponent_4_diag and curr_opponent_4_diag_ext:\n",
    "                reward += -0.15 * reward_scale  # Penalty for opponent's new extendable 4-consecutive (diagonal)\n",
    "            elif curr_opponent_3_diag and not prev_opponent_3_diag and curr_opponent_3_diag_ext:\n",
    "                reward += -0.10 * reward_scale  # Penalty for opponent's new extendable 3-consecutive (diagonal)\n",
    "            elif curr_opponent_2_diag and not prev_opponent_2_diag and curr_opponent_2_diag_ext:\n",
    "                reward += -0.03 * reward_scale  # Penalty for opponent's new 2-consecutive (diagonal)\n",
    "            \n",
    "            return True, reward\n",
    "        return False, 0.0\n",
    "    \n",
    "    def check_winner(self):\n",
    "            \"\"\"Check for a winner (4 horizontal/vertical or 5 diagonal) or draw.\"\"\"\n",
    "            # Check horizontal and vertical 4-consecutive\n",
    "            for i in range(BOARD_SIZE):\n",
    "                for j in range(BOARD_SIZE - 3):\n",
    "                    if abs(sum(self.data[i, j:j+4])) == 4:\n",
    "                        self.winner = self.data[i][j]\n",
    "                        self.end = True\n",
    "                        return\n",
    "                    if abs(sum(self.data[j:j+4, i])) == 4:\n",
    "                        self.winner = self.data[j][i]\n",
    "                        self.end = True\n",
    "                        return\n",
    "            \n",
    "            # Check diagonal 5-consecutive\n",
    "            for i in range(BOARD_SIZE - 4):\n",
    "                for j in range(BOARD_SIZE - 4):\n",
    "                    diag = [self.data[i+k][j+k] for k in range(5)]\n",
    "                    if abs(sum(diag)) == 5:\n",
    "                        self.winner = diag[0]\n",
    "                        self.end = True\n",
    "                        return\n",
    "                    anti_diag = [self.data[i+k][j+4-k] for k in range(5)]\n",
    "                    if abs(sum(anti_diag)) == 5:\n",
    "                        self.winner = anti_diag[0]\n",
    "                        self.end = True\n",
    "                        return\n",
    "            \n",
    "            # Check for draw (board full)\n",
    "            if np.all(self.data != 0):\n",
    "                self.end = True\n",
    "                self.winner = 0\n",
    "\n",
    "class EnhancedDQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network model for predicting action Q-values.\"\"\"\n",
    "    def __init__(self, hidden_size=512):\n",
    "        \"\"\"Initialize the DQN with convolutional and fully connected layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),  # Conv layer: 1 input channel, 64 filters\n",
    "            nn.BatchNorm2d(64),              # Batch normalization\n",
    "            nn.ReLU(),                       # ReLU activation\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  # Conv layer: 64 to 128 filters\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),  # Conv layer: 128 to 256 filters\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 * BOARD_SIZE * BOARD_SIZE, hidden_size),  # FC layer: flatten to hidden_size\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "            nn.Linear(hidden_size, BOARD_SIZE**2)  # Output: Q-values for 144 actions\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = self.conv(x.unsqueeze(1))  # Add channel dimension and apply conv layers\n",
    "        return self.fc(x.view(x.size(0), -1))  # Flatten and apply FC layers\n",
    "\n",
    "class RLAgent:\n",
    "    \"\"\"Reinforcement Learning agent using DQN.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the agent with main and target models, optimizer, and memory.\"\"\"\n",
    "        self.model = EnhancedDQN().to(device)  # Main DQN model\n",
    "        self.target_model = EnhancedDQN().to(device)  # Target DQN model\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Sync target with main\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-4, weight_decay=1e-5)  # AdamW optimizer\n",
    "        self.memory = deque(maxlen=50000)  # Experience replay memory\n",
    "        self.batch_size = 256  # Batch size for training\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.0  # Initial exploration rate\n",
    "        self.epsilon_min = 0.05  # Minimum exploration rate\n",
    "        self.epsilon_decay = 0.999  # Exploration decay rate\n",
    "        self.update_freq = 100  # Frequency to update target model\n",
    "        self.steps = 0  # Training steps counter\n",
    "        \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Select an action using epsilon-greedy or model Q-values.\"\"\"\n",
    "        valid_actions = self._get_valid_actions(state)  # Get valid moves\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        \n",
    "        if training and np.random.rand() < self.epsilon:\n",
    "            action = random.choice(valid_actions)  # Random action for exploration\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state.data).to(device)  # Convert state to tensor\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_tensor.unsqueeze(0)).squeeze()  # Get Q-values\n",
    "            mask = torch.full((BOARD_SIZE**2,), -np.inf, device=device)  # Mask invalid actions\n",
    "            mask[valid_actions] = q_values[valid_actions]\n",
    "            action = torch.argmax(mask).item()  # Select action with max Q-value\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def _get_valid_actions(self, state):\n",
    "        \"\"\"Get list of valid actions (indices of empty cells in valid zones).\"\"\"\n",
    "        return [i * BOARD_SIZE + j for i in range(BOARD_SIZE)\n",
    "                for j in range(BOARD_SIZE)\n",
    "                if state.is_valid_position(i, j) and state.data[i][j] == 0]\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition in the replay memory.\"\"\"\n",
    "        prev_state = State()\n",
    "        prev_state.data = np.copy(state.data)  # Copy current state\n",
    "        prev_state.end = state.end\n",
    "        prev_state.winner = state.winner\n",
    "        \n",
    "        next_state_copy = None\n",
    "        if next_state:\n",
    "            next_state_copy = State()\n",
    "            next_state_copy.data = np.copy(next_state.data)  # Copy next state\n",
    "            next_state_copy.end = next_state.end\n",
    "            next_state_copy.winner = next_state.winner\n",
    "        \n",
    "        # Store transition as tensors\n",
    "        self.memory.append((\n",
    "            torch.FloatTensor(prev_state.data).to(device),\n",
    "            action,\n",
    "            reward,\n",
    "            torch.FloatTensor(next_state_copy.data).to(device) if next_state_copy else None,\n",
    "            done\n",
    "        ))\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step using a batch from memory.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0  # Skip if not enough experiences\n",
    "        \n",
    "        # Sample a batch\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states = torch.stack([x[0] for x in batch]).to(device)\n",
    "        actions = torch.LongTensor([x[1] for x in batch]).to(device)\n",
    "        rewards = torch.FloatTensor([x[2] for x in batch]).to(device)\n",
    "        next_states = torch.stack([x[3] for x in batch if x[3] is not None]).to(device)\n",
    "        dones = torch.BoolTensor([x[4] for x in batch]).to(device)\n",
    "        \n",
    "        # Compute current Q-values\n",
    "        current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Compute target Q-values using target model\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.model(next_states).max(1)[1]\n",
    "            next_q = self.target_model(next_states).gather(1, next_actions.unsqueeze(1))\n",
    "        \n",
    "        # Calculate target Q-values\n",
    "        target_q = rewards.clone()\n",
    "        not_done = (~dones)\n",
    "        if not_done.any():\n",
    "            target_q[not_done] += self.gamma * next_q.squeeze()[:sum(not_done)]\n",
    "        \n",
    "        # Compute loss and optimize\n",
    "        loss = nn.SmoothL1Loss()(current_q.squeeze(), target_q.detach())\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update exploration rate\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Update target model periodically\n",
    "        if self.steps % self.update_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "class GameGUI:\n",
    "    \"\"\"Graphical User Interface for human-AI gameplay.\"\"\"\n",
    "    def __init__(self, master, ai_path=None):\n",
    "        \"\"\"Initialize the GUI with game state and AI agent.\"\"\"\n",
    "        self.master = master\n",
    "        self.state = State()  # Game state\n",
    "        self.ai = RLAgent()   # AI agent\n",
    "        if ai_path:\n",
    "            try:\n",
    "                self.ai.model.load_state_dict(torch.load(ai_path))  # Load trained model\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.ai.epsilon = 0.0  # Disable exploration for AI moves\n",
    "        \n",
    "        self.setup_ui()\n",
    "        \n",
    "    def setup_ui(self):\n",
    "        \"\"\"Set up the Tkinter canvas and event bindings.\"\"\"\n",
    "        self.master.title(\"Super Tic-Tac-Toe\")\n",
    "        self.canvas = tk.Canvas(self.master, \n",
    "                               width=CELL_SIZE*BOARD_SIZE,\n",
    "                               height=CELL_SIZE*BOARD_SIZE,\n",
    "                               bg=COLORS[\"bg\"])\n",
    "        self.canvas.pack()\n",
    "        self.draw_board()\n",
    "        self.canvas.bind(\"<Button-1>\", self.on_click)  # Bind mouse clicks\n",
    "        \n",
    "    def draw_board(self):\n",
    "        \"\"\"Draw the board with valid zones, grid, and pieces.\"\"\"\n",
    "        # Draw valid cross zones\n",
    "        for zone in CROSS_ZONES:\n",
    "            x1, x2, y1, y2 = zone\n",
    "            self.canvas.create_rectangle(\n",
    "                x1*CELL_SIZE, y1*CELL_SIZE,\n",
    "                x2*CELL_SIZE, y2*CELL_SIZE,\n",
    "                fill=COLORS[\"valid\"], outline=COLORS[\"grid\"])\n",
    "\n",
    "        # Draw grid lines\n",
    "        for i in range(BOARD_SIZE+1):\n",
    "            self.canvas.create_line(0, i*CELL_SIZE, \n",
    "                                  BOARD_SIZE*CELL_SIZE, i*CELL_SIZE,\n",
    "                                  fill=COLORS[\"grid\"])\n",
    "            self.canvas.create_line(i*CELL_SIZE, 0,\n",
    "                                  i*CELL_SIZE, BOARD_SIZE*BOARD_SIZE,\n",
    "                                  fill=COLORS[\"grid\"])\n",
    "\n",
    "        # Initialize piece ovals\n",
    "        self.pieces = {}\n",
    "        for i in range(BOARD_SIZE):\n",
    "            for j in range(BOARD_SIZE):\n",
    "                x = j*CELL_SIZE + CELL_SIZE//2\n",
    "                y = i*CELL_SIZE + CELL_SIZE//2\n",
    "                self.pieces[(i,j)] = self.canvas.create_oval(\n",
    "                    x-15, y-15, x+15, y+15,\n",
    "                    fill=COLORS[\"valid\"], outline=COLORS[\"valid\"])\n",
    "    \n",
    "    def update_display(self):\n",
    "        \"\"\"Update the display to reflect current board state.\"\"\"\n",
    "        for i in range(BOARD_SIZE):\n",
    "            for j in range(BOARD_SIZE):\n",
    "                color = COLORS[\"valid\"]\n",
    "                if self.state.data[i][j] == 1:\n",
    "                    color = COLORS[\"p1\"]  # Player 1 piece\n",
    "                elif self.state.data[i][j] == -1:\n",
    "                    color = COLORS[\"p2\"]  # Player 2 piece\n",
    "                self.canvas.itemconfig(self.pieces[(i,j)], fill=color, outline=color)\n",
    "    \n",
    "    def on_click(self, event):\n",
    "        \"\"\"Handle human player's mouse click.\"\"\"\n",
    "        if self.state.end:\n",
    "            return\n",
    "\n",
    "        col = event.x // CELL_SIZE\n",
    "        row = event.y // CELL_SIZE\n",
    "        \n",
    "        # Randomly perturb move (50% chance)\n",
    "        if np.random.rand() < 0.5:\n",
    "            final_row, final_col = row, col\n",
    "        else:\n",
    "            candidates = [(row + dr, col + dc) for dr in (-1,0,1) for dc in (-1,0,1) \n",
    "                        if not (dr == 0 and dc == 0)]\n",
    "            final_row, final_col = random.choice(candidates)\n",
    "        \n",
    "        valid = False\n",
    "        if 0 <= final_row < BOARD_SIZE and 0 <= final_col < BOARD_SIZE:\n",
    "            valid, _ = self.state.update_state(final_row, final_col, 1)  # Human move (Player 1)\n",
    "        \n",
    "        if valid:\n",
    "            self.update_display()\n",
    "            if self.state.end:\n",
    "                self.game_over()\n",
    "                return\n",
    "            self.master.after(500, self.ai_move)  # Schedule AI move after 500ms\n",
    "    \n",
    "    def ai_move(self):\n",
    "        \"\"\"Execute AI's move.\"\"\"\n",
    "        action = self.ai.get_action(self.state, training=False)  # Get AI action\n",
    "        if action is None:\n",
    "            return\n",
    "        \n",
    "        row = action // BOARD_SIZE\n",
    "        col = action % BOARD_SIZE\n",
    "        \n",
    "        # Randomly perturb move (50% chance)\n",
    "        if np.random.rand() < 0.5:\n",
    "            final_row, final_col = row, col\n",
    "        else:\n",
    "            candidates = [(row + dr, col + dc) for dr in (-1,0,1) for dc in (-1,0,1)\n",
    "                        if not (dr == 0 and dc == 0)]\n",
    "            final_row, final_col = random.choice(candidates)\n",
    "        \n",
    "        valid = False\n",
    "        if 0 <= final_row < BOARD_SIZE and 0 <= final_col < BOARD_SIZE:\n",
    "            valid, _ = self.state.update_state(final_row, final_col, -1)  # AI move (Player 2)\n",
    "        \n",
    "        if valid:\n",
    "            self.update_display()\n",
    "            if self.state.end:\n",
    "                self.game_over()\n",
    "    \n",
    "    def game_over(self):\n",
    "        \"\"\"Display game-over message and close GUI.\"\"\"\n",
    "        winner = \"Human\" if self.state.winner == 1 else \"AI\" if self.state.winner == -1 else \"Draw\"\n",
    "        messagebox.showinfo(\"Game Over\", f\"{winner} wins!\")\n",
    "        self.master.destroy()\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train the RL agent through self-play.\"\"\"\n",
    "    agent = RLAgent()\n",
    "    try:\n",
    "        for episode in range(1001):\n",
    "            state = State()\n",
    "            total_reward = 0\n",
    "            current_player = 1  # Start with Player 1\n",
    "            while not state.end:\n",
    "                prev_state = State()\n",
    "                prev_state.data = np.copy(state.data)  # Copy current state\n",
    "                \n",
    "                # Select action based on player\n",
    "                if current_player == 1:\n",
    "                    action = agent.get_action(state)  # AI move\n",
    "                else:\n",
    "                    valid_actions = agent._get_valid_actions(state)\n",
    "                    action = random.choice(valid_actions) if valid_actions else None  # Random opponent move\n",
    "                \n",
    "                if action is None:\n",
    "                    break\n",
    "                \n",
    "                row, col = divmod(action, BOARD_SIZE)\n",
    "                # Randomly perturb move (50% chance)\n",
    "                if np.random.rand() < 0.5:\n",
    "                    final_row, final_col = row, col\n",
    "                else:\n",
    "                    candidates = [(row + dr, col + dc) for dr in (-1,0,1) for dc in (-1,0,1)\n",
    "                                if not (dr == 0 and dc == 0)]\n",
    "                    final_row, final_col = random.choice(candidates)\n",
    "                \n",
    "                valid = False\n",
    "                reward = 0\n",
    "                if 0 <= final_row < BOARD_SIZE and 0 <= final_col < BOARD_SIZE:\n",
    "                    valid, reward = state.update_state(final_row, final_col, current_player)\n",
    "                \n",
    "                if valid:\n",
    "                    next_state = State()\n",
    "                    next_state.data = np.copy(state.data)  # Copy next state\n",
    "                    done = state.end\n",
    "                    \n",
    "                    # Store experience and train\n",
    "                    agent.store_experience(prev_state, action, reward, next_state, done)\n",
    "                    loss = agent.train_step()\n",
    "                    total_reward += reward\n",
    "                    \n",
    "                    current_player *= -1  # Switch player\n",
    "                    \n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode {episode}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "    finally:\n",
    "        torch.save(agent.model.state_dict(), \"best_model.pth\")  # Save trained model\n",
    "        print(\"Model saved to best_model.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    root = tk.Tk()\n",
    "    GameGUI(root, \"best_model.pth\")\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
